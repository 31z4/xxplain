"""
–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º xxplain –∫–ª–∞—Å—Å–æ–≤.
–ê–Ω–∞–ª–æ–≥ lab/build_datasets.py —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π.
"""

import os
import json
from pathlib import Path
from typing import List, Dict, Any, Optional
from dotenv import load_dotenv

from ml.experiments.dataset import DatasetBuilder
from ml.features.config import FeatureConfig, FULL_CONFIG


def setup_environment() -> str:
    """
    –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏ –ø–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î
    
    Returns:
        –°—Ç—Ä–æ–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è DSN
    """
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–∑ .env.postgres
    dotenv_path = Path(".env.postgres")
    load_dotenv(dotenv_path=dotenv_path)
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç—Ä–æ–∫—É –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
    dsn = os.environ.get(
        "PG_DSN",
        f"postgresql://postgres:{os.getenv('POSTGRES_PASSWORD')}@localhost:5432/postgres",
    )
    
    return dsn


def read_queries_from_source(source_path: str) -> List[str]:
    """
    –ß–∏—Ç–∞–µ—Ç SQL-–∑–∞–ø—Ä–æ—Å—ã –∏–∑ —Ñ–∞–π–ª–∞ –∏–ª–∏ –≤—Å–µ—Ö .sql —Ñ–∞–π–ª–æ–≤ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏.
    –°–æ–≤–º–µ—Å—Ç–∏–º–∞ —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–µ–π –∏–∑ lab/build_datasets.py
    
    Args:
        source_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∏–ª–∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å SQL –∑–∞–ø—Ä–æ—Å–∞–º–∏
        
    Returns:
        –°–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫ (–∑–∞–ø—Ä–æ—Å–æ–≤)
    """
    queries = []
    path_obj = Path(source_path)
    
    if path_obj.is_file():
        # –ï—Å–ª–∏ —ç—Ç–æ —Ñ–∞–π–ª, —á–∏—Ç–∞–µ–º –µ–≥–æ
        with open(path_obj, "r", encoding="utf-8") as f:
            lines = [line.strip() for line in f.readlines()]
            queries = [q for q in lines if q and not q.startswith("--")]
    
    elif path_obj.is_dir():
        # –ï—Å–ª–∏ —ç—Ç–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è, —á–∏—Ç–∞–µ–º –≤—Å–µ .sql —Ñ–∞–π–ª—ã
        sql_files = sorted(path_obj.glob("*.sql"))
        for file_path in sql_files:
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read().strip().replace("\n", " ")
                if content and not content.startswith("--"):
                    queries.append(content)
    else:
        raise ValueError(f"–ü—É—Ç—å {source_path} –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —Ñ–∞–π–ª–æ–º –∏–ª–∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–µ–π.")
    
    return queries


def create_feature_config() -> FeatureConfig:
    """
    –°–æ–∑–¥–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.
    –ê–Ω–∞–ª–æ–≥–∏—á–Ω–∞ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—É extract_features –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Å–∫—Ä–∏–ø—Ç–∞.
    
    Returns:
        –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    """
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–ª–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∫–∞–∫ –æ—Å–Ω–æ–≤—É –∏ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ø–æ–¥ –Ω–∞—à–∏ –Ω—É–∂–¥—ã
    config = FeatureConfig(
        # –ë–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–∞–Ω–∞–ª–æ–≥ original)
        extract_basic_stats=True,
        extract_node_counts=True,
        extract_join_info=True,
        extract_cost_estimates=True,
        
        # –ü—Ä–∏–∑–Ω–∞–∫–∏ –∑–∞–ø—Ä–æ—Å–∞
        extract_query_features=True,
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–µ—Ä–≤–µ—Ä–∞
        extract_pg_config=True,
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        extract_parallel_info=True,
        extract_filter_info=True,
        extract_aggregation_info=True,
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –æ—Ä–∏–≥–∏–Ω–∞–ª–æ–º
        normalize_costs=False,
        normalize_rows=False,
        
        # –ü—Ä–µ—Ñ–∏–∫—Å –ø—É—Å—Ç–æ–π
        feature_prefix=""
    )
    
    return config


def build_dataset_with_xxplain(
    queries: List[str],
    dsn: str,
    output_csv_path: str,
    timeout: int = 30,
    analyze: bool = True
) -> Dict[str, Any]:
    """
    –°—Ç—Ä–æ–∏—Ç –¥–∞—Ç–∞—Å–µ—Ç –∏—Å–ø–æ–ª—å–∑—É—è xxplain –∫–ª–∞—Å—Å—ã
    
    Args:
        queries: –°–ø–∏—Å–æ–∫ SQL –∑–∞–ø—Ä–æ—Å–æ–≤
        dsn: –°—Ç—Ä–æ–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î
        output_csv_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è CSV —Ñ–∞–π–ª–∞
        timeout: –¢–∞–π–º–∞—É—Ç –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        analyze: –í—ã–ø–æ–ª–Ω—è—Ç—å ANALYZE –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏
        
    Returns:
        –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞
    """
    print(f"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è DatasetBuilder...")
    
    # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    feature_config = create_feature_config()
    
    # –°–æ–∑–¥–∞–µ–º –±–∏–ª–¥–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞
    builder = DatasetBuilder(dsn=dsn, feature_config=feature_config)
    
    print(f"–ù–∞—á–∏–Ω–∞–µ–º –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑ {len(queries)} –∑–∞–ø—Ä–æ—Å–æ–≤...")
    print(f"–ê–Ω–∞–ª–∏–∑: {'–≤–∫–ª—é—á–µ–Ω' if analyze else '–æ—Ç–∫–ª—é—á–µ–Ω'}")
    print(f"–¢–∞–π–º–∞—É—Ç: {timeout} —Å–µ–∫—É–Ω–¥")
    print("." * 50)
    
    try:
        # –°—Ç—Ä–æ–∏–º –¥–∞—Ç–∞—Å–µ—Ç —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º timeout –∏ rollback
        # Timeout —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç—Å—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ —á–µ—Ä–µ–∑ statement_timeout
        # Rollback –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö –≤ SyncQueryPlanService
        dataset = builder.build_from_queries(
            queries=queries,
            output_path=output_csv_path,
            analyze=analyze,
            timeout=timeout
        )
        
        # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        stats = builder.get_dataset_statistics(dataset)
        
        print(f"\n–î–∞—Ç–∞—Å–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_csv_path}")
        print(f"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π: {stats.get('total_records', 0)}")
        print(f"–ó–∞–ø–∏—Å–∏ —Å —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π: {stats.get('records_with_target', 0)}")
        print(f"–ü–æ–∫—Ä—ã—Ç–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π: {stats.get('target_coverage', 0):.2%}")
        
        if 'target_stats' in stats and stats['target_stats']:
            target_stats = stats['target_stats']
            print(f"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è:")
            print(f"  –°—Ä–µ–¥–Ω–µ–µ: {target_stats.get('mean', 0):.2f} –º—Å")
            print(f"  –ú–µ–¥–∏–∞–Ω–∞: {target_stats.get('median', 0):.2f} –º—Å")
            print(f"  –ú–∏–Ω: {target_stats.get('min', 0):.2f} –º—Å")
            print(f"  –ú–∞–∫—Å: {target_stats.get('max', 0):.2f} –º—Å")
        
        if 'feature_stats' in stats:
            feature_stats = stats['feature_stats']
            print(f"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
            print(f"  –í—Å–µ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {feature_stats.get('total_features', 0)}")
            print(f"  –°—Ä–µ–¥–Ω–µ–µ –Ω–∞ –∑–∞–ø–∏—Å—å: {feature_stats.get('avg_features_per_record', 0):.1f}")
        
        return {
            "success": True,
            "records_processed": len(dataset),
            "output_file": output_csv_path,
            "statistics": stats
        }
        
    except Exception as e:
        print(f"\n–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞: {e}")
        return {
            "success": False,
            "error": str(e),
            "records_processed": 0
        }


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–æ–≤"""
    
    print("=== –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º xxplain ===\n")
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è
    try:
        dsn = setup_environment()
        print(f"–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î –Ω–∞—Å—Ç—Ä–æ–µ–Ω–æ")
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î: {e}")
        return
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö (–∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º—É —Å–∫—Ä–∏–ø—Ç—É)
    TRAIN_SQL_FILE = "benchmarks/tpc-h/generated/samples.sql"
    TEST_SQL_FOLDER = "benchmarks/tpc-h/queries/"
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è
    TRAIN_TIMEOUT = 20  # —Å–µ–∫—É–Ω–¥
    TEST_TIMEOUT = 100  # —Å–µ–∫—É–Ω–¥
    
    datasets_to_build = [
        {
            "name": "training",
            "source": TRAIN_SQL_FILE,
            "output": "datasets/train.csv",
            "timeout": TRAIN_TIMEOUT,
            "analyze": True
        },
        {
            "name": "test", 
            "source": TEST_SQL_FOLDER,
            "output": "datasets/test.csv",
            "timeout": TEST_TIMEOUT,
            "analyze": True
        }
    ]
    
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
    datasets_dir = Path("datasets")
    datasets_dir.mkdir(exist_ok=True)
    
    # –°—Ç—Ä–æ–∏–º –∫–∞–∂–¥—ã–π –¥–∞—Ç–∞—Å–µ—Ç
    results = {}
    
    for dataset_config in datasets_to_build:
        print(f"\n--- –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ {dataset_config['name']} –¥–∞—Ç–∞—Å–µ—Ç–∞ ---")
        
        try:
            # –ß–∏—Ç–∞–µ–º –∑–∞–ø—Ä–æ—Å—ã
            print(f"–ß—Ç–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤ –∏–∑: {dataset_config['source']}")
            queries = read_queries_from_source(dataset_config['source'])
            print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(queries)} –∑–∞–ø—Ä–æ—Å–æ–≤")
            
            if not queries:
                print(f"–ù–µ –Ω–∞–π–¥–µ–Ω–æ –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ {dataset_config['source']}")
                results[dataset_config['name']] = {"success": False, "error": "No queries found"}
                continue
            
            # –°—Ç—Ä–æ–∏–º –¥–∞—Ç–∞—Å–µ—Ç
            result = build_dataset_with_xxplain(
                queries=queries,
                dsn=dsn,
                output_csv_path=dataset_config['output'],
                timeout=dataset_config['timeout'],
                analyze=dataset_config['analyze']
            )
            
            results[dataset_config['name']] = result
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {dataset_config['name']} –¥–∞—Ç–∞—Å–µ—Ç–∞: {e}")
            results[dataset_config['name']] = {"success": False, "error": str(e)}
    
    # –ò—Ç–æ–≥–æ–≤–∞—è —Å–≤–æ–¥–∫–∞
    print(f"\n{'='*60}")
    print("–ò–¢–û–ì–û–í–ê–Ø –°–í–û–î–ö–ê:")
    print(f"{'='*60}")
    
    total_success = 0
    total_records = 0
    
    for name, result in results.items():
        status = "‚úì –£—Å–ø–µ—à–Ω–æ" if result.get("success") else "‚úó –û—à–∏–±–∫–∞"
        records = result.get("records_processed", 0)
        
        print(f"{name.upper()} –¥–∞—Ç–∞—Å–µ—Ç: {status}")
        if result.get("success"):
            print(f"  –ó–∞–ø–∏—Å–µ–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {records}")
            print(f"  –§–∞–π–ª: {result.get('output_file', 'N/A')}")
            total_success += 1
            total_records += records
        else:
            print(f"  –û—à–∏–±–∫–∞: {result.get('error', 'Unknown error')}")
        print()
    
    print(f"–£—Å–ø–µ—à–Ω–æ –ø–æ—Å—Ç—Ä–æ–µ–Ω–æ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤: {total_success}/{len(results)}")
    print(f"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π: {total_records}")
    
    if total_success == len(results):
        print("–í—Å–µ –¥–∞—Ç–∞—Å–µ—Ç—ã —É—Å–ø–µ—à–Ω–æ –ø–æ—Å—Ç—Ä–æ–µ–Ω—ã! üéâ")
    else:
        print("–ù–µ–∫–æ—Ç–æ—Ä—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –Ω–µ —É–¥–∞–ª–æ—Å—å. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –æ—à–∏–±–∫–∏ –≤—ã—à–µ.")


if __name__ == "__main__":
    main()
